{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Álvaro Sánchez Castañeda."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identificador de Idioma."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Nuestro objetivo es construir un algoritmo que clasifique un texto según el idioma en el que esté escrito. Para ello disponemos de corpus paralelos proporcionados por el Parlamento Europeo: http://www.statmt.org/europarl/. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Se ha creado un algoritmo que recibe un conjunto de entrenamiento formado por textos, y sus respectivos lenguajes. Sobre dicho conjunto \"entrena\"(modifica su comportamiento interno para poder predecir los elementos del conjunto de entrenamiento). Una vez \"entrenado\" el algoritmo, podemos usarlo para predecir el idioma de un nuevo fragmento de texto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tener en cuenta que, al estar usando texto sacado del parlamento europeo como entrenamiento y test, solo tenemos garantizado que este algoritmo funcionara bien sobre textos con caracteristicas similares: longitud corta de texto, terminologia, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Librerias y datos.\n",
    "\n",
    "En primer lugar, carguemos algunas librerias útiles para este problema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Lo siguiente que podemos hacer, es construir los conjuntos de entrenamiento y test. Una primera idea para trabajar con nuestros datos puede ser trabajar directamente sobre los corpus, el problema es que sabemos de que idioma es un texto por el corpus al que pertenece. Se puede trabajar así, pero parece mas natural definir una estructura que contenga cada elemento de texto, y su respectivo idioma. \n",
    "\n",
    " Se ha tratado de crear una base de datos mediante el uso de pandas, pero todos los procesos parecian muy costosos computacionalmente. De este modo, se ha optado por crear una lista formada a su vez por listas, las cuales contienen en la primera posición un fragmento de texto, y en la segunda el idioma al que pertenecen.\n",
    " \n",
    " Adjuntaré un script de python en el que esta el proceso para crear estas listas, pero aquí no nos centraremos en eso. Simplemente cargaremos las listas de entrenamiento y test creadas (estarán incluidas en el archivo que le envíe, dado que el conjunto test pesa mucho, le he enviado solo una parte). \n",
    " \n",
    " Es recomendable que cada idioma aparezca mas o menos el mismo numero de veces en el conjunto de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open (\"D:\\\\corpus_PLN1\\\\prueba_loop_borrar\\\\lista_train.txt\", 'rb') as fp:#Put the path of the train.\n",
    "    train = pickle.load(fp)\n",
    "    \n",
    "with open (\"D:\\\\corpus_PLN1\\\\prueba_loop_borrar\\\\lista_test.txt\", 'rb') as fp:#Put the path of the test.\n",
    "    test = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A su vez, necesitaremos una lista con los idiomas que vamos a usar en codigo ISO 639-1. Esto se usará internamente en el algoritmo, y servirá de referencia para usar las matrices de confusión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "languages=['en','bg','cs','da','de','el','es','et','fi','fr','hu','it','lv','nl','pl','pt','ro','sk','sl','sv']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algoritmo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pasemos a definir la clase que contendrá nuestro algoritmo. Dicha clase se basa en un diccionario con palabras sacadas de los corpus como claves, y listas de \"pesos\" en cada idioma (relaccionados con la lista languages) como valores.\n",
    "\n",
    "Para inicializar dicha clase tenemos que incluir los posibles idiomas (languages), el diccionario de los pesos (podemos tomarlo vacio, y que despues lo construya), y el metodo que usará el algoritmo por defecto para predecir (method),esto ultimo se explicara posteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Rosetta:    \n",
    "    def __init__(self, words,languages, method='sum'):            \n",
    "        #We save a count of words in a dictionary.\n",
    "        self.words = words\n",
    "        self.languages=languages\n",
    "        self.method=method\n",
    "             \n",
    "    def predict(self, p, method=None):\n",
    "        if method==None:\n",
    "            method=self.method\n",
    "        \n",
    "        if method=='sum':\n",
    "            return (self.predict_sum(p))\n",
    "        if method=='prob':\n",
    "            return (self.predict_prob(p))\n",
    "        if method=='abs':\n",
    "            return (self.predict_abs(p))        \n",
    "        \n",
    "    def predict_prob(self,p):\n",
    "        #This method multiply the probs of each word.\n",
    "        #predict_sum works better (problems with words with prob almost zero) \n",
    "        n_lgj=len(self.languages)\n",
    "        w_list= nltk.word_tokenize(p)\n",
    "        prob=[1 for i in range (n_lgj)]\n",
    "        for w in w_list:\n",
    "            if [x  for x in w if x in '.,123456789']==[]:\n",
    "                for w2 in self.words:\n",
    "                    if w==w2:\n",
    "                        prob_w=[min(x / sum(self.words[w2]), 0.01) for x in self.words[w2]]\n",
    "                        prob=[prob_w[i]*prob[i] for i in range(n_lgj)]\n",
    "        return self.languages[prob.index(max(prob))]\n",
    "                            \n",
    "    def predict_sum(self,p):\n",
    "        #This method sum the probs of each word.\n",
    "        n_lgj=len(self.languages)\n",
    "        w_list= nltk.word_tokenize(p)\n",
    "        prob=[1 for i in range (n_lgj)]\n",
    "        for w in w_list:\n",
    "            if [x  for x in w if x in '.,123456789']==[]:\n",
    "                for w2 in self.words:\n",
    "                    if w==w2:\n",
    "                        prob_w=[x / sum(self.words[w2]) for x in self.words[w2]]\n",
    "                        prob=[prob_w[i]+prob[i] for i in range(n_lgj)]\n",
    "        return self.languages[prob.index(max(prob))]\n",
    "\n",
    "    def predict_abs(self,p):\n",
    "        #This method works using the absolute frequency..\n",
    "        n_lgj=len(self.languages)\n",
    "        w_list= nltk.word_tokenize(p)\n",
    "        prob=[1 for i in range (n_lgj)]\n",
    "        for w in w_list:\n",
    "            if [x  for x in w if x in '.,123456789']==[]:\n",
    "                for w2 in self.words:\n",
    "                    if w==w2:\n",
    "                        prob_w=self.words[w2]\n",
    "                        prob=[prob_w[i]+prob[i] for i in range(n_lgj)]\n",
    "        return self.languages[prob.index(max(prob))]\n",
    "    \n",
    "                            \n",
    "    def predict_retro(self,p, method=None):\n",
    "        if method==None:\n",
    "            method=self.method\n",
    "        #This method sum the probs of each word.\n",
    "        #Also, we tray to improve te model when we predict.\n",
    "        \n",
    "        #In this block we predict.\n",
    "        n_lgj=len(self.languages)\n",
    "        w_list= nltk.word_tokenize(p)\n",
    "        prob=[1 for i in range (n_lgj)]\n",
    "        pred=self.predict(p, method=method)\n",
    "        pos_lgj=self.languages.index(pred)\n",
    "        \n",
    "        #In this block we 'train'.\n",
    "        for w in w_list:\n",
    "            if [x  for x in w if x in '.,123456789']==[]:\n",
    "                if w not in (self.words).keys():\n",
    "                    self.words [w]=[0 if i!= pos_lgj else 1 for i in range(n_lgj)]\n",
    "                else:\n",
    "                    self.words [w][pos_lgj]+=1\n",
    "        \n",
    "        return self.languages[prob.index(max(prob))] \n",
    "\n",
    "    def predict_by_phrases(self,t,method=None):\n",
    "        #We have a text t, we do the mean the predictions of each prhase in t.\n",
    "        if method==None:\n",
    "            method=self.method\n",
    "        p_list=nltk.sent_tokenize(t)\n",
    "        ph_list=[]\n",
    "        for p in p_list:\n",
    "            ph_list=ph_list+[x   for x in p.split('\"') if x != '']\n",
    "        pred_list=[]\n",
    "        for p in ph_list:\n",
    "            pred_list.append(self.predict(p,method=method))\n",
    "        d=nltk.FreqDist(pred_list)\n",
    "        return (max(d, key=d.get),[[i,j]for i,j in zip (ph_list,pred_list)])\n",
    "         \n",
    "    def pseudo_train(self,data):\n",
    "        n_lgj=len(self.languages)\n",
    "        for l in data:\n",
    "            for w in nltk.word_tokenize(l[0]):\n",
    "                pos_lgj=self.languages.index(l[1])\n",
    "                if w not in (self.words).keys():\n",
    "                    self.words [w]=[0 if i!= pos_lgj else 1 for i in range(n_lgj)]\n",
    "                else:\n",
    "                    self.words [w][pos_lgj]+=1\n",
    "      \n",
    "    def train(self, data,a=1, method=None):\n",
    "        #a is the number wich is added or substracted in the dictionary words\n",
    "        start_time = time.time()\n",
    "        if method==None:\n",
    "            method=self.method\n",
    "        n_lgj=len(self.languages)\n",
    "        for l in data:\n",
    "            real_ln=l[1]\n",
    "            pos_lgj=self.languages.index(real_ln)\n",
    "            pred_ln=self.predict(l[0], method=method)\n",
    "            if real_ln != pred_ln:\n",
    "                for w in nltk.word_tokenize(l[0]):\n",
    "            \n",
    "                    if w not in (self.words).keys():\n",
    "                        self.words [w]=[0 if i!= pos_lgj else a for i in range(n_lgj)]\n",
    "                    else:\n",
    "                        self.words [w][pos_lgj]+= 2*a\n",
    "                        for i in range(n_lgj):\n",
    "                            if self.words [w][i] >= a:\n",
    "                                self.words [w][i]+= -a\n",
    "                            else:\n",
    "                                self.words [w][i]=0\n",
    "                                \n",
    "            else:\n",
    "                for w in nltk.word_tokenize(l[0]):\n",
    "            \n",
    "                    if w not in (self.words).keys():\n",
    "                        self.words [w]=[0 if i!= pos_lgj else a for i in range(n_lgj)]\n",
    "                    else:\n",
    "                        self.words [w][pos_lgj]+= a\n",
    "        print('Se han tardado ',(time.time() - start_time)/60,' minutos.')\n",
    "        \n",
    "\n",
    "    def test_mc(self, data ,method=None):\n",
    "        start_time = time.time()\n",
    "        if method==None:\n",
    "            method=self.method\n",
    "        n_lgj=len(self.languages)\n",
    "        mc=[[0 for i in range(n_lgj)] for i in range(n_lgj)]\n",
    "        for l in data:\n",
    "            real_ln=l[1]\n",
    "            pred_ln=self.predict(l[0],method=method)\n",
    "            mc[self.languages.index(real_ln)][self.languages.index(pred_ln)] +=1\n",
    "        print('Se han tardado ',(time.time() - start_time)/60,' minutos.')\n",
    "        return (mc)\n",
    "    \n",
    "        \n",
    "    def test_mc_by_phrases(self, data,method=None):\n",
    "        start_time = time.time()\n",
    "        if method==None:\n",
    "            method=self.method\n",
    "        start_time = time.time()\n",
    "        n_lgj=len(self.languages)\n",
    "        mc=[[0 for i in range(n_lgj)] for i in range(n_lgj)]\n",
    "        for l in data:\n",
    "            real_ln=l[1]\n",
    "            pred_ln=self.predict_by_phrases(l[0], method=method)[0]\n",
    "            mc[self.languages.index(real_ln)][self.languages.index(pred_ln)] +=1\n",
    "        print('Se han tardado ',(time.time() - start_time)/60,' minutos.')\n",
    "        return (mc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explicación del algoritmo.\n",
    "\n",
    "Tenemos tres tipos de funciones: predictoras (predict), de entrenamiento (train), y unas ultimas para construir la matriz de confusión sobre datos test (test_mc).\n",
    "\n",
    "Tenemos tres metodos de predicción. 'sum':Para las palabras del texto a predecir, sumamos cada peso(contenido en el diccionario words) dividido por la suma de los pesos de dicha palabra para cada uno de los posibles idiomas. Una vez echo esto para cada idioma, clasificamos el texto como aquel que tiene una suma mayor. Si el diccionario contiene las frecuencias absolutas de las palabras para cada idioma, este metodo simplemente suma las frecuencias relativas de dicha palabra para cada idioma.\n",
    "\n",
    "'prob': funciona igual que 'sum', salvo por que en lugar de sumar los pesos, los multiplicamos. Si el diccionario contiene las frecuencias absolutas de las palabras para cada idioma, estamos calculando las probabilidades de que la frase pertenezca a cada idioma (asumiendo que la aparicion de dos palabras cuales quiera son sucesos independientes, lo cual no es cierto). El metodo 'sum' funciona mejor.\n",
    "\n",
    "'abs': Funciona igual que sum, salvo que no dividimos los pesos que aparecen en el diccionario, simplemente los sumamos. Es el que peor funciona, pues dará mas peso a las palabras que mas aparecen, las cuales no tienen por que ser las mas determinantes para clasificar.\n",
    "\n",
    " Por ultimo, hay definidas otras dos funciones predictoras: Predict_retro ademas de predecir, modifica los pesos de las palabras del texto a clasificar, aumentandolos para el idioma predicho, y disminuyendolos para el resto. El objetivo es que se retroalimente el algoritmo. Por ejemplo, si tenemos una frase con una palabra desconocida, pero la clasifica bien. Es razonable pensar que esa palabra pertenece a dicho idioma, y así lo \"aprenderá\" el algoritmo.\n",
    " \n",
    " La ultima función predictora clasifica cada una de las frases del texto a predecir. El objetivo seria buscar menciones que aparezcan en otros idiomas y cosas por el estilo.\n",
    " \n",
    " Probemos algunos de estos algoritmos predictores, pero antes necesitaremos usar el metodo pseudo_train (que calcula las frecuencias absolutas de las palabras de train en cada idioma)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ej=Rosetta({},languages=languages)\n",
    "ej.pseudo_train(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'es'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ej.predict('Hola mundo.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'en'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ej.predict('Hello wordld.', method='prob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fr'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ej.predict('Salut monde.', method='abs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por defecto usamos el metodo 'sum'. \n",
    "\n",
    "El algoritmo ya funciona, pero podemos tratar de mejorarlo usando un diccionario cuyos pesos se modifiquen tratando de predecir bien el conjunto train, en vez de contener las frecuencias absolutas. Para esto definimos el metodo train, recive el conjunto de entrenamiento (data), el metodo de predicción (method), y un numero que se usara para modificar los pesos (a). Para cada texto del conjunto de entrenamiento predecimos (usando el metodo que queramos), si acertamos añadimos a al peso de cada una de las palabras para el idioma del texto. Si fallamos, añadimos a a los pesos del idioma real, y restamos a al resto.\n",
    "\n",
    "Podemos entrenar directamente el modelo inicializado con el diccionario vacio, o podemos entrenarlo partiendo del diccionario de las frecuencias absolutas.\n",
    "\n",
    "Lo recomendable es usar el conjunto train completo, pero como el objetivo ahora es ilustrar el funcionamiento de la clase, podemos tomar un subconjunto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se han tardado  4.433675726254781  minutos.\n"
     ]
    }
   ],
   "source": [
    "train_aux=[ train[i] for i in random.sample(list(range(len(train))), 1000)]\n",
    "ej.train(train_aux, a=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos usar un conjunto test y ver que tal predice este modelo viendo su matriz de confusión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se han tardado  4.70621060927709  minutos.\n"
     ]
    }
   ],
   "source": [
    "test_aux=[ test[i] for i in random.sample(list(range(len(test))), 1000)]\n",
    "mc=ej.test_mc(test_aux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['en', 'bg', 'cs', 'da', 'de', 'el', 'es', 'et', 'fi', 'fr', 'hu', 'it', 'lv', 'nl', 'pl', 'pt', 'ro', 'sk', 'sl', 'sv']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[77, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 23, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 15, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 77, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 63, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 51, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 90, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 27, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 1, 63, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 76, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 15, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 70, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 27, 0, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 97, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 20, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 70, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 13, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 24, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 24, 0],\n",
       " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 67]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(languages)\n",
    "mc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos definir una función que de el porcentaje de acierto atraves de la matriz de confusión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tasa_acierto(mc):\n",
    "    error=sum([ mc[i][j]  for  i in range(len(languages)) for j in range (len(mc[0])) if i != j])\n",
    "    acierto=sum([ mc[i][j]  for  i in range(len(languages)) for j in range (len(mc[0])) if i == j])  \n",
    "    return (acierto/(acierto+error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.989"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tasa_acierto(mc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por ultimo, veamos como funciona el metodo predict_by_phrases: \n",
    "\n",
    "Este nos devuelve una lista de dos elementos, el segundo elemento a su vez es una lista que contiene fragmentos del texto, y predicciones del idioma de los respectivos fragmentos. Para averiguar cual es idioma del texto completo, tomamos el idioma mas frecuente para los fragmentos, y lo devolvemos en la primera posición de la lista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('es', [['Hola, ¿que tal estas?.', 'es'], [\"I'm really well.\", 'en']])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ej.predict_by_phrases(\"Hola, ¿que tal estas?. I'm really well.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('es', [['El nos dijo: ', 'es'], ['hey, what are you doing?', 'en']])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ej.predict_by_phrases('El nos dijo: \"hey, what are you doing?\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos una matriz de confusion resultante de aplicar este metodo sobre un conjunto test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se han tardado  4.688563215732574  minutos.\n"
     ]
    }
   ],
   "source": [
    "mc2=ej.test_mc_by_phrases(test_aux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.985\n",
      "['en', 'bg', 'cs', 'da', 'de', 'el', 'es', 'et', 'fi', 'fr', 'hu', 'it', 'lv', 'nl', 'pl', 'pt', 'ro', 'sk', 'sl', 'sv']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[77, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 23, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 15, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 74, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2],\n",
       " [0, 0, 0, 0, 63, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 51, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 90, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 27, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 1, 63, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 76, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 15, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 70, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 27, 0, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 97, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 20, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 70, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 13, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 24, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 24, 0],\n",
       " [1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 66]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tasa_acierto(mc2))\n",
    "print(languages)\n",
    "mc2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultados con un conjunto test de tamaño 10000.\n",
    "\n",
    "Por ultimo, entrenemos el modelo con todos los datos train, y usemos una mayor cantidad de datos test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se han tardado  36.62121630907059  minutos.\n",
      "Se han tardado  47.006208284695944  minutos.\n",
      "0.9903\n",
      "['en', 'bg', 'cs', 'da', 'de', 'el', 'es', 'et', 'fi', 'fr', 'hu', 'it', 'lv', 'nl', 'pl', 'pt', 'ro', 'sk', 'sl', 'sv']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[774, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0],\n",
       " [0, 165, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [2, 0, 248, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1],\n",
       " [6, 0, 0, 762, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 2],\n",
       " [0, 0, 0, 0, 709, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0],\n",
       " [0, 0, 0, 0, 0, 457, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0],\n",
       " [5, 0, 0, 0, 0, 0, 751, 0, 0, 0, 0, 0, 0, 1, 0, 4, 0, 0, 1, 1],\n",
       " [3, 0, 0, 0, 0, 0, 0, 257, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       " [6, 0, 0, 1, 0, 0, 0, 0, 692, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       " [4, 0, 0, 0, 0, 0, 0, 0, 0, 754, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0],\n",
       " [2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 248, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 743, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 264, 0, 0, 0, 0, 0, 0, 0],\n",
       " [5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 739, 0, 0, 0, 0, 1, 1],\n",
       " [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 265, 0, 0, 0, 0, 0],\n",
       " [4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 742, 0, 0, 1, 1],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 152, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 260, 1, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 242, 0],\n",
       " [9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 679]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algoritmo=Rosetta({},languages=languages)\n",
    "algoritmo.train(train, a=3)\n",
    "mc=algoritmo.test_mc(test)\n",
    "print(tasa_acierto(mc))\n",
    "print(languages)\n",
    "mc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos la matriz de confusión, pero usando predict_by_phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se han tardado  47.099146687984465  minutos.\n",
      "0.9876\n",
      "['en', 'bg', 'cs', 'da', 'de', 'el', 'es', 'et', 'fi', 'fr', 'hu', 'it', 'lv', 'nl', 'pl', 'pt', 'ro', 'sk', 'sl', 'sv']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[774, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0],\n",
       " [0, 165, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [2, 0, 248, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1],\n",
       " [11, 0, 1, 748, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 3, 7],\n",
       " [1, 0, 0, 0, 708, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0],\n",
       " [0, 0, 0, 0, 0, 455, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1],\n",
       " [5, 0, 0, 0, 1, 0, 750, 0, 0, 0, 0, 0, 0, 1, 0, 4, 0, 0, 1, 1],\n",
       " [3, 0, 0, 0, 0, 0, 0, 257, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       " [6, 0, 0, 1, 0, 0, 0, 0, 692, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       " [5, 0, 0, 0, 0, 0, 0, 0, 0, 753, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0],\n",
       " [2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 248, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 743, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 263, 0, 0, 0, 0, 0, 0, 0],\n",
       " [7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 737, 0, 0, 0, 0, 1, 1],\n",
       " [2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 262, 0, 0, 0, 0, 1],\n",
       " [4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 742, 0, 0, 1, 1],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 152, 0, 0, 0],\n",
       " [1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 259, 1, 0],\n",
       " [2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 241, 0],\n",
       " [9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 679]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mc_p=algoritmo.test_mc_by_phrases(test)\n",
    "print(tasa_acierto(mc_p))\n",
    "print(languages)\n",
    "mc_p"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
